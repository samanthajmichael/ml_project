{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samanthajmichael/ml_project/blob/main/notebooks/Base_Model_Training_with_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorflow ffmpeg-python opencv-python matplotlib"
      ],
      "metadata": {
        "id": "wt6BRYhRcAg7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvd-_3AEchng",
        "outputId": "0fd06e74-f4fa-44fa-b630-304398cbd6af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "D-ecWosbcRII"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Get GPU device information\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    for device in gpu_devices:\n",
        "        # Get device details\n",
        "        device_details = tf.config.experimental.get_device_details(device)\n",
        "        print(\"GPU Details:\", device_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzWW-fdmqeJa",
        "outputId": "31c5aa5b-8dd1-4831-97cb-bdc4b02cf4d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU Details: {'compute_capability': (7, 5), 'device_name': 'Tesla T4'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yluJc9nwb6Td"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_frame(frame_data, target_size=(224, 224)):\n",
        "    # Convert to TensorFlow tensors earlier in the pipeline\n",
        "    nparr = np.frombuffer(frame_data, np.uint8)\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, target_size)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    # Convert to TensorFlow tensor here\n",
        "    return tf.convert_to_tensor(img)\n",
        "\n",
        "def data_generator(df, batch_size):\n",
        "    while True:\n",
        "        batch = df.sample(n=batch_size)\n",
        "        # Create tensor batch directly\n",
        "        frames = tf.stack([load_and_preprocess_frame(frame_data)\n",
        "                         for frame_data in batch['frame_data']])\n",
        "        yield frames\n",
        "\n",
        "@tf.function  # Add tf.function for faster execution\n",
        "def augment_frame(frame):\n",
        "    # Ensure input is a tensor\n",
        "    frame = tf.convert_to_tensor(frame)\n",
        "    frame = tf.image.random_flip_left_right(frame)\n",
        "    frame = tf.image.random_brightness(frame, max_delta=0.1)\n",
        "    frame = tf.image.random_contrast(frame, lower=0.9, upper=1.1)\n",
        "    frame = tf.clip_by_value(frame, 0.0, 1.0)\n",
        "    return frame\n",
        "\n",
        "def create_base_model(input_shape=(224, 224, 3)):\n",
        "    base_model = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    # Add batch normalization for better training stability\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    projection_head = Dense(128, activation='relu')(x)\n",
        "    # Add L2 normalization layer with explicit output shape\n",
        "    projection_head = tf.keras.layers.Lambda(\n",
        "        lambda x: tf.math.l2_normalize(x, axis=1),\n",
        "        output_shape=lambda input_shape: input_shape  # Explicitly specify output shape\n",
        "    )(projection_head)\n",
        "    return Model(inputs=base_model.input, outputs=projection_head)\n",
        "\n",
        "@tf.function\n",
        "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
        "    # Ensure inputs are float32\n",
        "    z_i = tf.cast(z_i, tf.float32)\n",
        "    z_j = tf.cast(z_j, tf.float32)\n",
        "\n",
        "    z_i = tf.math.l2_normalize(z_i, axis=1)\n",
        "    z_j = tf.math.l2_normalize(z_j, axis=1)\n",
        "    similarity_matrix = tf.matmul(z_i, z_j, transpose_b=True) / temperature\n",
        "\n",
        "    batch_size = tf.shape(z_i)[0]\n",
        "    contrastive_labels = tf.range(batch_size)\n",
        "\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=contrastive_labels,\n",
        "        logits=similarity_matrix\n",
        "    )\n",
        "    return tf.cast(tf.reduce_mean(loss), tf.float32)\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, optimizer, x):\n",
        "    x_i = augment_frame(x)\n",
        "    x_j = augment_frame(x)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        z_i = model(x_i, training=True)\n",
        "        z_j = model(x_j, training=True)\n",
        "        loss = nt_xent_loss(z_i, z_j)\n",
        "        # Ensure loss is float32\n",
        "        loss = tf.cast(loss, tf.float32)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train_contrastive_model(df, epochs=1, batch_size=15, steps_per_epoch=50):\n",
        "    # Use float32 instead of mixed precision to avoid dtype issues\n",
        "    model = create_base_model()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    generator = tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(df, batch_size),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(batch_size, 224, 224, 3), dtype=tf.float32)\n",
        "        )\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Initialize total_loss as float32\n",
        "        total_loss = tf.constant(0.0, dtype=tf.float32)\n",
        "\n",
        "        for step, x in enumerate(generator.take(steps_per_epoch)):\n",
        "            loss = train_step(model, optimizer, x)\n",
        "            # Ensure both are same dtype before adding\n",
        "            total_loss += tf.cast(loss, tf.float32)\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                tf.print(f\"Epoch {epoch+1}/{epochs}, Step {step}, Loss: {loss:.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / tf.cast(steps_per_epoch, tf.float32)\n",
        "        tf.print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.save(f'model_checkpoint_epoch_{epoch+1}', save_format='tf')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/ML Project/middle_15min_frames.pkl\")"
      ],
      "metadata": {
        "id": "sU5j48fhcVmO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n",
        "print(f\"DataFrame shape: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUXbVHyodEuT",
        "outputId": "8068ebfc-cea2-48dc-be5e-ef0a109839da"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20700 entries, 0 to 20699\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   frame_number  20700 non-null  int64 \n",
            " 1   frame_data    20700 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 323.6+ KB\n",
            "None\n",
            "DataFrame shape: (20700, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate frame numbers\n",
        "print(\"\\nFrame number statistics:\")\n",
        "print(df['frame_number'].describe())\n",
        "\n",
        "# Check for duplicate frame numbers\n",
        "duplicates = df['frame_number'].duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate frame numbers: {duplicates}\")\n",
        "\n",
        "# Display a few rows of the DataFrame\n",
        "print(\"\\nFirst few rows of the DataFrame:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRPzkmGqdI2I",
        "outputId": "9c10e508-4f7c-400a-adb8-0889ba8e8f6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frame number statistics:\n",
            "count     20700.000000\n",
            "mean      92814.500000\n",
            "std        5975.719622\n",
            "min       82465.000000\n",
            "25%       87639.750000\n",
            "50%       92814.500000\n",
            "75%       97989.250000\n",
            "max      103164.000000\n",
            "Name: frame_number, dtype: float64\n",
            "\n",
            "Number of duplicate frame numbers: 0\n",
            "\n",
            "First few rows of the DataFrame:\n",
            "   frame_number                                         frame_data\n",
            "0         82465  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "1         82466  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "2         82467  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "3         82468  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "4         82469  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_contrastive_model(df, epochs=4, batch_size=30, steps_per_epoch=50)\n",
        "trained_model.save('/content/drive/MyDrive/ML Project/trained_base_model_4_epochs.h5')\n",
        "print(\"Training completed and model saved.\")"
      ],
      "metadata": {
        "id": "U2ckBxBuctCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ab0a94-ebcc-479a-c618-392e1e59e4c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4, Step 0, Loss: 2.9318\n",
            "Epoch 1/4, Step 10, Loss: 2.2292\n",
            "Epoch 1/4, Step 20, Loss: 2.2271\n",
            "Epoch 1/4, Step 30, Loss: 2.1055\n",
            "Epoch 1/4, Step 40, Loss: 2.0562\n",
            "Epoch 1/4, Avg Loss: 2.2067\n",
            "Epoch 2/4, Step 0, Loss: 2.0387\n",
            "Epoch 2/4, Step 10, Loss: 2.2244\n",
            "Epoch 2/4, Step 20, Loss: 2.1796\n",
            "Epoch 2/4, Step 30, Loss: 2.2425\n",
            "Epoch 2/4, Step 40, Loss: 2.0691\n",
            "Epoch 2/4, Avg Loss: 2.1328\n",
            "Epoch 3/4, Step 0, Loss: 2.0479\n",
            "Epoch 3/4, Step 10, Loss: 2.2810\n",
            "Epoch 3/4, Step 20, Loss: 2.2711\n",
            "Epoch 3/4, Step 30, Loss: 2.1422\n",
            "Epoch 3/4, Step 40, Loss: 2.1256\n",
            "Epoch 3/4, Avg Loss: 2.1572\n",
            "Epoch 4/4, Step 0, Loss: 2.0949\n",
            "Epoch 4/4, Step 10, Loss: 2.2712\n",
            "Epoch 4/4, Step 20, Loss: 2.3063\n",
            "Epoch 4/4, Step 30, Loss: 2.2005\n",
            "Epoch 4/4, Step 40, Loss: 2.2037\n",
            "Epoch 4/4, Avg Loss: 2.1899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed and model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hChb1JtvKowx",
        "outputId": "3376a42d-cce2-4808-f7d8-72dc95c87ad3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Functional name=functional, built=True>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}