{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samanthajmichael/ml_project/blob/main/notebooks/Base_Model_Training_with_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorflow ffmpeg-python opencv-python matplotlib"
      ],
      "metadata": {
        "id": "wt6BRYhRcAg7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvd-_3AEchng",
        "outputId": "c8fd8b4c-5582-486e-bba8-d4c4b0582af9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "D-ecWosbcRII"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Get GPU device information\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    for device in gpu_devices:\n",
        "        # Get device details\n",
        "        device_details = tf.config.experimental.get_device_details(device)\n",
        "        print(\"GPU Details:\", device_details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzWW-fdmqeJa",
        "outputId": "7a8f85c9-8904-4d1e-83f4-66a73bc4a1ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU Details: {'compute_capability': (7, 5), 'device_name': 'Tesla T4'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yluJc9nwb6Td"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_frame(frame_data, target_size=(224, 224)):\n",
        "    # Your existing preprocessing code remains the same\n",
        "    nparr = np.frombuffer(frame_data, np.uint8)\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, target_size)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return tf.convert_to_tensor(img)\n",
        "\n",
        "@tf.function\n",
        "def augment_frame(frame):\n",
        "    # Enhanced data augmentation\n",
        "    frame = tf.image.random_flip_left_right(frame)\n",
        "    frame = tf.image.random_brightness(frame, max_delta=0.2)\n",
        "    frame = tf.image.random_contrast(frame, lower=0.8, upper=1.2)\n",
        "    frame = tf.image.random_saturation(frame, lower=0.8, upper=1.2)\n",
        "    # Add random rotation\n",
        "    frame = tf.image.rot90(frame, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
        "    frame = tf.clip_by_value(frame, 0.0, 1.0)\n",
        "    return frame\n",
        "\n",
        "class ContrastiveLearningModel(tf.keras.Model):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super(ContrastiveLearningModel, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.base_model = self._create_base_model()\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
        "\n",
        "    def _create_base_model(self, input_shape=(224, 224, 3)):\n",
        "        base_model = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
        "        x = GlobalAveragePooling2D()(base_model.output)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = Dense(256, activation='relu')(x)  # Increased projection head size\n",
        "        x = tf.keras.layers.Dropout(0.3)(x)   # Added dropout\n",
        "        projection_head = Dense(128, activation=None)(x)\n",
        "        projection_head = tf.keras.layers.Lambda(\n",
        "            lambda x: tf.math.l2_normalize(x, axis=1)\n",
        "        )(projection_head)\n",
        "        return Model(inputs=base_model.input, outputs=projection_head)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return self.base_model(inputs, training=training)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x = data\n",
        "        batch_size = tf.shape(x)[0]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Generate two augmented views\n",
        "            x_i = augment_frame(x)\n",
        "            x_j = augment_frame(x)\n",
        "\n",
        "            # Get embeddings\n",
        "            z_i = self.base_model(x_i, training=True)\n",
        "            z_j = self.base_model(x_j, training=True)\n",
        "\n",
        "            # Compute NT-Xent loss\n",
        "            loss = self._nt_xent_loss(z_i, z_j)\n",
        "\n",
        "        # Compute and apply gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "    def _nt_xent_loss(self, z_i, z_j):\n",
        "        # Your existing NT-Xent loss implementation\n",
        "        z_i = tf.math.l2_normalize(z_i, axis=1)\n",
        "        z_j = tf.math.l2_normalize(z_j, axis=1)\n",
        "        similarity_matrix = tf.matmul(z_i, z_j, transpose_b=True) / self.temperature\n",
        "\n",
        "        batch_size = tf.shape(z_i)[0]\n",
        "        contrastive_labels = tf.range(batch_size)\n",
        "\n",
        "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=contrastive_labels,\n",
        "            logits=similarity_matrix\n",
        "        )\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "def train_contrastive_model(df, epochs=10, batch_size=32, steps_per_epoch=50):\n",
        "    # Create dataset\n",
        "    generator = tf.data.Dataset.from_generator(\n",
        "        lambda: data_generator(df, batch_size),\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(batch_size, 224, 224, 3), dtype=tf.float32)\n",
        "        )\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Create model and callbacks\n",
        "    model = ContrastiveLearningModel()\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    lr_scheduler = ReduceLROnPlateau(\n",
        "        monitor='loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # TensorBoard callback for monitoring\n",
        "    tensorboard = TensorBoard(\n",
        "        log_dir='./logs',\n",
        "        histogram_freq=1,\n",
        "        write_graph=True,\n",
        "        update_freq='epoch'\n",
        "    )\n",
        "\n",
        "    # Compile model with cosine decay learning rate\n",
        "    initial_learning_rate = 0.001\n",
        "    decay_steps = epochs * steps_per_epoch\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate, decay_steps\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    )\n",
        "        # Train model\n",
        "    history = model.fit(\n",
        "        generator,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=[lr_scheduler, early_stopping, tensorboard],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "df = pd.read_pickle(\"/content/drive/MyDrive/ML Project/middle_15min_frames.pkl\")"
      ],
      "metadata": {
        "id": "sU5j48fhcVmO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())\n",
        "print(f\"DataFrame shape: {df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUXbVHyodEuT",
        "outputId": "29a6f2f2-f6b8-431f-a3e2-7084c68f38ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20700 entries, 0 to 20699\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   frame_number  20700 non-null  int64 \n",
            " 1   frame_data    20700 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 323.6+ KB\n",
            "None\n",
            "DataFrame shape: (20700, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate frame numbers\n",
        "print(\"\\nFrame number statistics:\")\n",
        "print(df['frame_number'].describe())\n",
        "\n",
        "# Check for duplicate frame numbers\n",
        "duplicates = df['frame_number'].duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate frame numbers: {duplicates}\")\n",
        "\n",
        "# Display a few rows of the DataFrame\n",
        "print(\"\\nFirst few rows of the DataFrame:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRPzkmGqdI2I",
        "outputId": "b09d3f1b-da9f-49b8-bf0d-3ec5c513d2d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frame number statistics:\n",
            "count     20700.000000\n",
            "mean      92814.500000\n",
            "std        5975.719622\n",
            "min       82465.000000\n",
            "25%       87639.750000\n",
            "50%       92814.500000\n",
            "75%       97989.250000\n",
            "max      103164.000000\n",
            "Name: frame_number, dtype: float64\n",
            "\n",
            "Number of duplicate frame numbers: 0\n",
            "\n",
            "First few rows of the DataFrame:\n",
            "   frame_number                                         frame_data\n",
            "0         82465  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "1         82466  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "2         82467  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "3         82468  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
            "4         82469  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_contrastive_model(df, epochs=4, batch_size=30, steps_per_epoch=50)\n",
        "trained_model.save('/content/drive/MyDrive/ML Project/trained_base_model_4_epochs.keras')\n",
        "print(\"Training completed and model saved.\")"
      ],
      "metadata": {
        "id": "U2ckBxBuctCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77336988-339f-4499-b80d-126feed5e83c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4, Step 0, Loss: 2.9618\n",
            "Epoch 1/4, Step 10, Loss: 2.2845\n",
            "Epoch 1/4, Step 20, Loss: 2.1266\n",
            "Epoch 1/4, Step 30, Loss: 2.2418\n",
            "Epoch 1/4, Step 40, Loss: 2.0911\n",
            "Epoch 1/4, Avg Loss: 2.2394\n",
            "Epoch 2/4, Step 0, Loss: 2.1812\n",
            "Epoch 2/4, Step 10, Loss: 2.1067\n",
            "Epoch 2/4, Step 20, Loss: 2.1725\n",
            "Epoch 2/4, Step 30, Loss: 2.0990\n",
            "Epoch 2/4, Step 40, Loss: 2.1842\n",
            "Epoch 2/4, Avg Loss: 2.1657\n",
            "Epoch 3/4, Step 0, Loss: 2.0956\n",
            "Epoch 3/4, Step 10, Loss: 2.6612\n",
            "Epoch 3/4, Step 20, Loss: 2.3105\n",
            "Epoch 3/4, Step 30, Loss: 2.2161\n",
            "Epoch 3/4, Step 40, Loss: 2.0678\n",
            "Epoch 3/4, Avg Loss: 2.1911\n",
            "Epoch 4/4, Step 0, Loss: 2.1215\n",
            "Epoch 4/4, Step 10, Loss: 2.0872\n",
            "Epoch 4/4, Step 20, Loss: 2.0267\n",
            "Epoch 4/4, Step 30, Loss: 2.1604\n",
            "Epoch 4/4, Step 40, Loss: 2.0768\n",
            "Epoch 4/4, Avg Loss: 2.1182\n",
            "Training completed and model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hChb1JtvKowx",
        "outputId": "8b5701ed-cb4e-4d72-d4ff-d5ccb8cae1ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Functional name=functional, built=True>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}